{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk Like IDEO: similarity scores\n",
    "- Look at which articles in our dataset have similar language to what IDEO uses\n",
    "- 1. Create a single document that combines all IDEO Journal articles\n",
    "- 2. Early experimentation gensim dictionaries, BOW models, TF-IDF, and Doc2Vec models\n",
    "- 3. Creates scatter plots with cosine similarity to IDEO reference and to \"good leads\" refrence \n",
    "\n",
    "\n",
    "\n",
    "## NOTE: \n",
    "### This was an analysis path we chose not to pursue further\n",
    "### a lot of these functions have been improved in subsequent notebooks! (especially notebook #9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:55:41.175289Z",
     "start_time": "2020-11-20T19:55:40.296466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:55:43.756742Z",
     "start_time": "2020-11-20T19:55:41.189213Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora.textcorpus import TextCorpus\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:55:43.831864Z",
     "start_time": "2020-11-20T19:55:43.777665Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_stopwords(stopwords_path = \"../1_data/stopwords/stopwords.txt\"):\n",
    "    with open(stopwords_path, \"r\") as f:\n",
    "        sw = f.read().split()\n",
    "    return STOPWORDS.union(set(sw))\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "print(f\"{len(stopwords)} stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T04:47:16.435835Z",
     "start_time": "2020-11-10T04:47:16.392190Z"
    }
   },
   "source": [
    "## Load IDEO docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:55:44.200927Z",
     "start_time": "2020-11-20T19:55:43.838840Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_dir_text_files(data_dir, verbose=True):\n",
    "\n",
    "    skiplist = ['.DS_Store', \"index.txt\", \"index.csv\", \"index\"]\n",
    "\n",
    "#     articles = {}\n",
    "    files = []\n",
    "    texts = []\n",
    "    p = pathlib.Path(data_dir)\n",
    "    for article_path in p.glob('*'):\n",
    "        if article_path.is_dir():\n",
    "            continue\n",
    "        fname = article_path.name\n",
    "        fname = fname.split('.txt')[0]\n",
    "        if fname in skiplist:\n",
    "            continue\n",
    "        txt = article_path.read_text()\n",
    "        files.append(fname)\n",
    "        texts.append(txt)\n",
    "    if verbose:\n",
    "        print(f\"{len(texts)} docs found in {data_dir}\")\n",
    "    return files, texts\n",
    "\n",
    "def preprocess(text, stopwords=stopwords):\n",
    "    tokens = gensim.utils.simple_preprocess(text, \n",
    "                                            deacc=True,\n",
    "                                            min_len=3,\n",
    "                                           )\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# set the path\n",
    "ideo_path = '../1_data/IDEO_journal'\n",
    "\n",
    "# read all the files\n",
    "ideo_titles, ideo_docs = read_dir_text_files(ideo_path)\n",
    "\n",
    "# combine into one big text block\n",
    "ideo_journal_single_big_text = \"\\n\".join(ideo_docs)\n",
    "\n",
    "# preprocess into tokens\n",
    "ideo_doc_tokens = preprocess(ideo_journal_single_big_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Good_Leads docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:55:44.521341Z",
     "start_time": "2020-11-20T19:55:44.209240Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the path\n",
    "gl_path = '../1_data/good_lead_articles/'\n",
    "\n",
    "# read all the files\n",
    "gl_files, gl_docs = read_dir_text_files(gl_path)\n",
    "\n",
    "# combine into one big text block\n",
    "gl_single_big_text = \"\\n\".join(gl_docs)\n",
    "\n",
    "# preprocess into tokens\n",
    "gl_tokens = preprocess(gl_single_big_text)\n",
    "len(gl_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess all the documents\n",
    "\n",
    "So far it's just get their text and tokenize it\n",
    "\n",
    "also save the titles and directories of the docs (for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:55:47.676840Z",
     "start_time": "2020-11-20T19:55:44.528844Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_files_from_dir(paths, verbose=True):\n",
    "\n",
    "    all_files, all_docs, all_paths = [], [], []\n",
    "    for p in paths:\n",
    "        files, docs = read_dir_text_files(p, verbose=verbose)\n",
    "        all_files += files\n",
    "        all_docs += docs\n",
    "        all_paths +=  ([p.split('/')[-2]] * len(docs)) # repeates the name of the directory for each item\n",
    "        \n",
    "    return all_files, all_docs, all_paths\n",
    "\n",
    "\n",
    "# grab all the files from these directories and make some tokens\n",
    "input_paths = [\n",
    "        \"../1_data/how_i_built_this/\",\n",
    "#         \"../1_data/example_articles/\",\n",
    "        \"../1_data/NYT_corner_office/\",\n",
    "        \"../1_data/good_lead_articles/\",\n",
    "        \"../1_data/current_transformational_client_articles/\",\n",
    "        ]\n",
    "\n",
    "files, docs, paths = add_files_from_dir(input_paths)\n",
    "tokens = [preprocess(doc) for doc in docs]\n",
    "extended_tokens = ([preprocess(doc) for doc in ideo_docs] + \n",
    "                   [preprocess(doc) for doc in gl_docs] + \n",
    "                   tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an index \n",
    "It's going to be a dataframe that stores:\n",
    "- additional information about a document - interviewee, title, source, year, etc. \n",
    "- similarity scores\n",
    "- file paths (temporary link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:03:24.711526Z",
     "start_time": "2020-11-20T20:03:24.645084Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:03:47.900249Z",
     "start_time": "2020-11-20T20:03:47.839263Z"
    }
   },
   "outputs": [],
   "source": [
    "# str(df_index[df_index['year'].isna()].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:37.341880Z",
     "start_time": "2020-11-20T20:04:37.149861Z"
    }
   },
   "outputs": [],
   "source": [
    "def assemble_index_files(input_paths):\n",
    "    indexes = []\n",
    "    for p in input_paths:\n",
    "#         dirname = pathlib.Path(p).name\n",
    "        index_path = pathlib.Path(f\"{p}/index.csv\")\n",
    "        if not index_path.exists():\n",
    "            print(f\"index.csv does not exist in {p}\")\n",
    "\n",
    "        df_index_part = pd.read_csv(index_path)\n",
    "#         df_index_part['group'] = dirname\n",
    "        print(p, len(df_index_part))\n",
    "        indexes.append(df_index_part)\n",
    "\n",
    "\n",
    "    df_index = pd.concat(indexes, axis=0)\n",
    "    fnames = [str(f).split('.txt')[0] for f in df_index['filename']]\n",
    "    df_index['filename'] = fnames\n",
    "    return df_index.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "def check_filename_index_conflicts(df_index):\n",
    "    \n",
    "    # reporting\n",
    "    before_dedupe = df_index['filename'].value_counts()\n",
    "    dups = []\n",
    "    for k, v in before_dedupe.items():\n",
    "        if v > 1:\n",
    "            dups.append((k, v))\n",
    "    if len(dups):\n",
    "        print(f'{len(dups)} filenames with overlapping index')\n",
    "        for d in dups:\n",
    "            print(f\"{d[0]}: {d[1]} references\")\n",
    "\n",
    "        print('\\nremoving duplicates from index')\n",
    "        return df_index.drop_duplicates(subset=['filename'], keep=\"last\")\n",
    "    \n",
    "    return df_index\n",
    "            \n",
    "\n",
    "# load all the index files\n",
    "df_index_a = assemble_index_files(input_paths)\n",
    "\n",
    "df_index_a['year'] = df_index_a['year'].fillna(0).astype(int)\n",
    "\n",
    "new_titles = []\n",
    "df_index_a.rename(columns={'title': 'headline'}, inplace=True)\n",
    "for i, row in df_index_a.iterrows():\n",
    "    name = row['name']\n",
    "    year = row['year']\n",
    "    new_titles.append(f\"{name} ({year})\")\n",
    "df_index_a['title'] = new_titles\n",
    "\n",
    "# df_index_a['year'] = df_index_a['year'].astype(int)\n",
    "# Watch out! Duplicates!\n",
    "#The how I built this is messy! there's a couple episodes with the same people that then overwrite their files.\n",
    "#quickfix is I'm removing them from the index for now. longer fix would be to actually fix things and give them slightly different names\n",
    "\n",
    "    \n",
    "df_index_a = check_filename_index_conflicts(df_index_a)\n",
    "df_index_a.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:32:04.480246Z",
     "start_time": "2020-11-20T16:32:04.418763Z"
    }
   },
   "source": [
    "## get index to reflect the order of document, text, token lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:38.277402Z",
     "start_time": "2020-11-20T20:04:38.180677Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is a temporary df used to create a match between:\n",
    "# 1) names of the files \n",
    "# 2) text in (docs and tokens list)\n",
    "# The order in which documents are loaded might not reflect the order in the df_index\n",
    "df_index_b = pd.DataFrame([files, paths], index=['filename', 'group']).T\n",
    "df_index_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:38.475955Z",
     "start_time": "2020-11-20T20:04:38.393349Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicate filenames\n",
    "df_index_b[df_index_b['filename'].duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:38.840295Z",
     "start_time": "2020-11-20T20:04:38.774713Z"
    }
   },
   "outputs": [],
   "source": [
    "files_not_in_indexed = list(set(df_index_b['filename']) - set(df_index_a['filename']))\n",
    "index_without_files = list( set(df_index_a['filename']) - set(df_index_b['filename']))\n",
    "overlap = list( set(df_index_a['filename']) | set(df_index_b['filename']))\n",
    "\n",
    "\n",
    "print(f'{len(files_not_in_indexed)} files not referenced by an index.csv')\n",
    "print(f'{len(index_without_files)} indexes missing a file')\n",
    "print()\n",
    "print(f'{len(overlap)} overlaps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:39.078654Z",
     "start_time": "2020-11-20T20:04:39.002451Z"
    }
   },
   "outputs": [],
   "source": [
    "# i = pd.read_csv('../1_data/current_transformational_client_articles/index.csv')\n",
    "# i\n",
    "df_index_a.set_index('filename').loc[index_without_files]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:39.566462Z",
     "start_time": "2020-11-20T20:04:39.510260Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_index = df_index_b.set_index('filename').loc[files_not_in_indexed].sort_values(by='group')\n",
    "# missing_index[missing_index['dir'] == \"current_transformational_client_articles\"]\n",
    "missing_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:39.777661Z",
     "start_time": "2020-11-20T20:04:39.723568Z"
    }
   },
   "outputs": [],
   "source": [
    "df_index_a.set_index('filename').loc[index_without_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:58:05.637980Z",
     "start_time": "2020-11-20T15:58:05.554327Z"
    }
   },
   "source": [
    "### Merge to create final index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:40.139543Z",
     "start_time": "2020-11-20T20:04:40.102167Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:40.341289Z",
     "start_time": "2020-11-20T20:04:40.296098Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_index = df_index_b.set_index(\"filename\").join(df_index_a.set_index('filename'), how='left')\n",
    "\n",
    "a = df_index_a.set_index('filename')\n",
    "b = df_index_b.set_index(\"filename\")\n",
    "\n",
    "df_index = b.join(a, how='left')\n",
    "df_index.shape, df_index_b.shape, df_index_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:40.904002Z",
     "start_time": "2020-11-20T20:04:40.836962Z"
    }
   },
   "outputs": [],
   "source": [
    "df_index_b[df_index_b['filename'].duplicated()]\n",
    "# df_index_b[df_index_b['filename'] == 'marvin_chiefexecutive_2018']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spotcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:41.454916Z",
     "start_time": "2020-11-20T20:04:41.416083Z"
    }
   },
   "outputs": [],
   "source": [
    "index_order = df_index.index.to_list()\n",
    "file_order = df_index_b['filename'].to_list()\n",
    "for i in range(len(df_index)):\n",
    "    filename1 = index_order[i]\n",
    "    filename2 = file_order[i]\n",
    "    if filename1 != filename2:\n",
    "        print('ERROR', i, filename1, filename2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:41.754023Z",
     "start_time": "2020-11-20T20:04:41.704941Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def check_doc_index_match():\n",
    "\n",
    "#     d = 'good_lead_articles'\n",
    "#     subsample = df_index[df_index['group'] == d]\n",
    "    subsample = df_index\n",
    "    random_article = random.sample(list(subsample.index), 1)[0]\n",
    "    i = df_index.index.get_loc(random_article)\n",
    "    print(i, random_article)\n",
    "    print(df_index.loc[random_article, 'name'])\n",
    "    print('---')\n",
    "    print(docs[i])\n",
    "    \n",
    "check_doc_index_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:26:18.150466Z",
     "start_time": "2020-11-20T19:26:18.081040Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:24:49.984163Z",
     "start_time": "2020-11-20T19:24:49.908126Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T00:41:52.327161Z",
     "start_time": "2020-11-17T00:41:52.261856Z"
    }
   },
   "source": [
    "## Make gensim dictionary\n",
    "\n",
    "gensim object that lets us do doc2bow functions.\n",
    "\n",
    "Keeps track of tokens + ids for each token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:42.718951Z",
     "start_time": "2020-11-20T20:04:42.612855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Could combine\n",
    "full_dictionary = gensim.corpora.Dictionary([ideo_doc_tokens, gl_tokens])\n",
    "# Right now this is just the ideo words and Good Leads words...\n",
    "\n",
    "# add all the tokens from the documents?\n",
    "# full_dictionary.add_documents(tokens)\n",
    "\n",
    "len(full_dictionary), type(full_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T00:49:53.034076Z",
     "start_time": "2020-11-17T00:49:52.958714Z"
    }
   },
   "source": [
    "## Make Corpus object & Similarity Model\n",
    "\n",
    "corpus is a list where each element is a BOW (list with word frequencies)\n",
    "\n",
    "Term Frequency – Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:43.437553Z",
     "start_time": "2020-11-20T20:04:43.107151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Bag of Words for IDEO + Corpus\n",
    "ideo_bow = full_dictionary.doc2bow(ideo_doc_tokens)\n",
    "gl_bow = full_dictionary.doc2bow(gl_tokens)\n",
    "corpus = [full_dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "# Add IDEO to the corpus and mark it's postion\n",
    "full_corpus = [ideo_bow, gl_bow] + corpus\n",
    "ref_names = ['ideo', 'gl']\n",
    "ref_indicies= [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:45.211051Z",
     "start_time": "2020-11-20T20:04:43.440679Z"
    }
   },
   "outputs": [],
   "source": [
    "def similarity_to_refs(corpus, ref_indicies=[0,1]):\n",
    "\n",
    "    sims = gensim.similarities.Similarity(output_prefix = 'workdir/',\n",
    "                                          corpus=corpus,\n",
    "                                          num_features=len(full_dictionary))\n",
    "    \n",
    "    sim_to_corpus = []\n",
    "    sim_between_refs = []\n",
    "    for i in ref_indicies:\n",
    "        reference_array = corpus[i]\n",
    "        similarity_to_refrence = sims[reference_array]\n",
    "        assert similarity_to_refrence[i] > 0.99\n",
    "        clean_similarity = np.delete(similarity_to_refrence, ref_indicies)\n",
    "        sim_to_corpus.append(clean_similarity)\n",
    "        sim_between_refs.append(similarity_to_refrence[ref_indicies]) \n",
    "        \n",
    "    return sim_to_corpus, sim_between_refs\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "sim_name = 'bow'\n",
    "new_cols = [f\"sim_{sim_name}_{ref_name}\" for ref_name in ref_names]\n",
    "sims, ref_sims  = similarity_to_refs(corpus=full_corpus, ref_indicies=ref_indicies)\n",
    "for col,sim  in zip(new_cols, sims):\n",
    "    df_index[col] = sim\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    sns.histplot(sims[0], ax=ax)\n",
    "    ax.set_title(col)\n",
    "    plt.show()\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sims[0],sims[1], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T21:38:58.334079Z",
     "start_time": "2020-11-19T21:38:57.997259Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:04:47.694046Z",
     "start_time": "2020-11-20T20:04:45.215437Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "tf_idf = gensim.models.TfidfModel(full_corpus)\n",
    "\n",
    "\n",
    "sim_name = 'tfidf'\n",
    "transformed_corpus = tf_idf[full_corpus]\n",
    "\n",
    "\n",
    "new_cols = [f\"sim_{sim_name}_{ref_name}\" for ref_name in ref_names]\n",
    "sims, ref_sims  = similarity_to_refs(corpus=transformed_corpus, ref_indicies=ref_indicies)\n",
    "for col,sim  in zip(new_cols, sims):\n",
    "    df_index[col] = sim\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    sns.histplot(sims[0], ax=ax)\n",
    "    ax.set_title(col)\n",
    "    plt.show()\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sims[0],sims[1], '.')\n",
    "ax.set_xlabel('sim to ideo')\n",
    "ax.set_ylabel('sim to leads')\n",
    "plt.savefig('similarity comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:00:23.473706Z",
     "start_time": "2020-11-20T20:00:23.403496Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:05:48.530340Z",
     "start_time": "2020-11-20T20:05:48.258714Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_interactive_scatter(df, x, y, savename=None):\n",
    "\n",
    "    color_col = 'group'\n",
    "\n",
    "    selection = alt.selection_multi(fields=[color_col])\n",
    "\n",
    "    color = alt.condition(selection,\n",
    "                          alt.Color(f'{color_col}:N', legend=None),\n",
    "                          alt.value('lightgray'))\n",
    "\n",
    "    scatter = alt.Chart(df_index).mark_circle(size=60).encode(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        color=color,\n",
    "        href='url',\n",
    "        tooltip=[\"title\", 'headline', 'group', 'url']\n",
    "    ).interactive()\n",
    "\n",
    "\n",
    "    legend = alt.Chart(df_index).mark_point().encode(\n",
    "        y=alt.Y(f'{color_col}:N', axis=alt.Axis(orient='right')),\n",
    "        color=color\n",
    "    ).add_selection(\n",
    "        selection\n",
    "    )\n",
    "\n",
    "    if savename is not None:\n",
    "        (scatter | legend).save(savename)\n",
    "    \n",
    "    return scatter | legend\n",
    "\n",
    "create_interactive_scatter(df=df_index, x='sim_tfidf_ideo', y='sim_tfidf_gl', \n",
    "                           savename='tfidf.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:05:49.207146Z",
     "start_time": "2020-11-20T20:05:49.166507Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_index.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:58:10.513309Z",
     "start_time": "2020-11-20T15:58:10.469310Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually pretty bad correlation between similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:05:51.876133Z",
     "start_time": "2020-11-20T20:05:51.838585Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(df_index['similarity_bow'], df_index['similarity_tfidf'], '.')\n",
    "# # ax.set_xlim(0, 1)\n",
    "# # ax.set_ylim(0,1)\n",
    "# ax.set_ylabel('tf idf')\n",
    "# ax.set_xlabel('bow word counts')\n",
    "# ax.set_title('correlation between similarities')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mess with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:05:52.830808Z",
     "start_time": "2020-11-20T20:05:52.793628Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:15:08.928430Z",
     "start_time": "2020-11-20T20:15:08.871892Z"
    }
   },
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=t, tags=[str(i)]) for i, t in enumerate(tokens)]\n",
    "len(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:26:20.807185Z",
     "start_time": "2020-11-20T20:24:17.392297Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =0)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v_dm0_short.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:28:42.742776Z",
     "start_time": "2020-11-20T20:28:42.209187Z"
    }
   },
   "outputs": [],
   "source": [
    "def similarity(doc_vectors, ref_vector):\n",
    "    return [1 - cosine(ref_vector, doc_vectors[i]) for i in range(len(tokens))]\n",
    "\n",
    "\n",
    "\n",
    "# model_file = \"d2v_short.model\"\n",
    "# model_file = \"d2v_long.model\"\n",
    "model_file = \"d2v_dm0_short.model\"\n",
    "# model_file = \"d2v_dm0_long.model\"\n",
    "\n",
    "sname = model_file.split(\".\")[0]\n",
    "\n",
    "model= Doc2Vec.load(model_file)\n",
    "\n",
    "\n",
    "ideo_vector = model.infer_vector(ideo_doc_tokens)\n",
    "gl_vector = model.infer_vector(gl_tokens)\n",
    "\n",
    "y_name = 'sim_doc2vec_ideo'\n",
    "x_name = 'sim_doc2vec_gl'\n",
    "\n",
    "df_index[x_name] = similarity(doc_vectors=model.docvecs, ref_vector=ideo_vector)\n",
    "df_index[y_name] = similarity(doc_vectors=model.docvecs, ref_vector=gl_vector)\n",
    "\n",
    "\n",
    "create_interactive_scatter(df=df_index, x=x_name, y=y_name, \n",
    "                           savename=f'{sname}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:26:44.580116Z",
     "start_time": "2020-11-20T20:26:44.491739Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(docs[0]), len(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:32:22.346527Z",
     "start_time": "2020-11-20T20:32:22.057525Z"
    }
   },
   "outputs": [],
   "source": [
    "x_name = 'sim_doc2vec_ideo'\n",
    "y_name = 'sim_tfidf_ideo'\n",
    "\n",
    "sname = 'ideo_similarity_diff'\n",
    "\n",
    "create_interactive_scatter(df=df_index, x=x_name, y=y_name, \n",
    "                           savename=f'{sname}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:32:41.826666Z",
     "start_time": "2020-11-20T20:32:41.632192Z"
    }
   },
   "outputs": [],
   "source": [
    "x_name = 'sim_doc2vec_gl'\n",
    "y_name = 'sim_tfidf_gl'\n",
    "\n",
    "sname = 'gl_similarity_diff'\n",
    "\n",
    "create_interactive_scatter(df=df_index, x=x_name, y=y_name, \n",
    "                           savename=f'{sname}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:37:36.144139Z",
     "start_time": "2020-11-20T20:37:36.045448Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_index\n",
    "char_counts = []\n",
    "token_counts = []\n",
    "unique_tokens = []\n",
    "\n",
    "for i, (doc, tok) in enumerate(zip(docs, tokens)):\n",
    "    char_counts.append(len(doc))\n",
    "    token_counts.append(len(tok))\n",
    "    unique_tokens.append(len(set(tok)))\n",
    "\n",
    "df_index['char_count'] = char_counts\n",
    "df_index['token_counts'] = token_counts\n",
    "df_index['unique_token'] = unique_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T20:40:19.471904Z",
     "start_time": "2020-11-20T20:40:19.210470Z"
    }
   },
   "outputs": [],
   "source": [
    "# sub_df = df_index[df_index['group'] == 'good_lead_articles']\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(sub_df['char_count'], sub_df['sim_tfidf_gl'], '.')\n",
    "# plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sub_df['token_counts'], sub_df['sim_tfidf_gl'], '.')\n",
    "plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(sub_df['token_counts'], sub_df['char_count'], '.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz Customization Code\n",
    "## TODO: add to python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/altair-viz/altair/issues/1422\n",
    "\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "two_charts_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/vega@{vega_version}\"></script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@{vegalite_version}\"></script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@{vegaembed_version}\"></script>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<div id=\"vis1\"></div>\n",
    "<div id=\"vis2\"></div>\n",
    "\n",
    "<script type=\"text/javascript\">\n",
    "  vegaEmbed('#vis1', {spec1}).catch(console.error);\n",
    "  vegaEmbed('#vis2', {spec2}).catch(console.error);\n",
    "</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'x': range(5), 'y': range(5)})\n",
    "\n",
    "chart1 = alt.Chart(df).mark_point().encode(x='x', y='y')\n",
    "chart2 = alt.Chart(df).mark_line().encode(x='x', y='y')\n",
    "\n",
    "with open('charts.html', 'w') as f:\n",
    "    f.write(two_charts_template.format(\n",
    "        vega_version=alt.VEGA_VERSION,\n",
    "        vegalite_version=alt.VEGALITE_VERSION,\n",
    "        vegaembed_version=alt.VEGAEMBED_VERSION,\n",
    "        spec1=chart1.to_json(indent=None),\n",
    "        spec2=chart2.to_json(indent=None),\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
